{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Patent Owner Cleaning & Harmonization Pipeline\n",
        "Author: Anna-Pauliina Kokko\n",
        "Purpose: Production-grade pipeline for Lens.org exports.\n",
        "Logic decoupled from sensitive and confidential data.\n",
        "\"\"\"\n",
        "\n",
        "import os, json, csv, hashlib, datetime, unicodedata, re\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "# --- 1. SETTINGS & EXTERNAL CONFIG ---\n",
        "SETTINGS = {\n",
        "    \"INPUT_FILE\": \"input_data.xlsx\",\n",
        "    \"OUTPUT_BASENAME\": \"Cleaned_Dataset\",\n",
        "    \"FUZZY_THRESHOLD\": 87,\n",
        "    \"DIACRITIC_MODE\": \"broad\",\n",
        "    \"APPLY_FUZZY_TO_MAIN\": False\n",
        "}\n",
        "\n",
        "# Load External Knowledge Bases (Prevents Confidentiality Leaks)\n",
        "def load_config(file_path: str, default_val):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f) if file_path.endswith('.json') else pd.read_csv(file_path)\n",
        "    return default_val\n",
        "\n",
        "# Externalized data files\n",
        "HARMONIZATION_RULES = load_config(\"harmonization_map.csv\", pd.DataFrame())\n",
        "PARENT_PATTERNS = load_config(\"parent_patterns.json\", {})\n",
        "COMPANY_CUES = load_config(\"company_cues.json\", [\"oy\", \"ab\", \"inc\", \"ltd\"]) # Generic defaults\n",
        "\n",
        "# --- 2. TEXT NORMALIZATION ---\n",
        "def normalize_unicode(s: str) -> str:\n",
        "    \"\"\"Standardizes punctuation and normalizes to NFKC.\"\"\"\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.replace(\"–\", \"-\").replace(\"’\", \"'\").replace(\"”\", '\"')\n",
        "    return unicodedata.normalize(\"NFKC\", s)\n",
        "\n",
        "def restore_diacritics(token: str) -> str:\n",
        "    \"\"\"Handles Nordic character restoration based on SETTINGS.\"\"\"\n",
        "    if SETTINGS[\"DIACRITIC_MODE\"] == \"broad\":\n",
        "        token = re.sub(r\"oe\", \"ö\", token)\n",
        "        token = re.sub(r\"ae\", \"ä\", token)\n",
        "    return token\n",
        "\n",
        "# --- 3. CORE CLEANING ENGINE ---\n",
        "def clean_token(token: str) -> Optional[str]:\n",
        "    \"\"\"Applies multi-stage cleaning: lowercase, suffix removal, and diacritics.\"\"\"\n",
        "    if not token: return None\n",
        "    x = normalize_unicode(token).lower()\n",
        "    x = re.sub(r\"\\([^)]*\\)\", \" \", x) # Remove parentheses\n",
        "    # Generic legal suffixes\n",
        "    suffix_pattern = r\"\\b(oy|oyj|ab|abp|gmbh|ltd|inc|plc|corp|sa|spa|as|aps)\\b\"\n",
        "    x = re.sub(suffix_pattern, \" \", x, flags=re.IGNORECASE)\n",
        "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
        "    return restore_diacritics(x) or None\n",
        "\n",
        "# --- 4. CLASSIFICATION & HIERARCHY ---\n",
        "def classify_entity(name: str) -> str:\n",
        "    \"\"\"Classifies entity based on keyword cues.\"\"\"\n",
        "    n = name.lower()\n",
        "    if any(cue in n for cue in [\"university\", \"college\", \"institute\"]): return \"academia\"\n",
        "    if any(cue in n for cue in [\"foundation\", \"stiftung\", \"säätiö\"]): return \"nonprofit\"\n",
        "    if any(cue in n for cue in COMPANY_CUES): return \"company\"\n",
        "    return \"individual\"\n",
        "\n",
        "def detect_parent(name: str) -> str:\n",
        "    \"\"\"Maps cleaned names to parent entities using external regex patterns.\"\"\"\n",
        "    for parent, patterns in PARENT_PATTERNS.items():\n",
        "        if any(re.search(p, name, re.I) for p in patterns):\n",
        "            return parent\n",
        "    return name\n",
        "\n",
        "# --- 5. EXECUTION PIPELINE ---\n",
        "def run_pipeline():\n",
        "    df = pd.read_excel(SETTINGS[\"INPUT_FILE\"])\n",
        "\n",
        "    # Process Owners & Applicants\n",
        "    for col in [\"Owners\", \"Applicants\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"{col.lower()}_clean\"] = df[col].apply(\n",
        "                lambda x: [clean_token(p) for p in str(x).split(\";\")]\n",
        "            )\n",
        "\n",
        "    # Apply Classification and Parent Detection\n",
        "    df[\"category\"] = df[\"owners_clean\"].apply(lambda lst: [classify_entity(x) for x in lst if x])\n",
        "    df[\"parent_entity\"] = df[\"owners_clean\"].apply(lambda lst: [detect_parent(x) for x in lst if x])\n",
        "\n",
        "    # Export with unique Dataset ID for version control\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "    df.to_excel(f\"{SETTINGS['OUTPUT_BASENAME']}_{timestamp}.xlsx\", index=False)\n",
        "    print(f\"Success: Processed {len(df)} rows.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "G5lVR74DrhKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}