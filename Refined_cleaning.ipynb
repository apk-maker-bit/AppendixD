{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZq0VPMMQLeoiayP0wc5J6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apk-maker-bit/AppendixD/blob/source-code/Refined_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Patent Owner Cleaning & Harmonization Pipeline\n",
        "Author: Anna-Pauliina Kokko\n",
        "Purpose: Production-grade pipeline for Lens.org exports.\n",
        "Logic decoupled from sensitive and confidential data.\n",
        "\"\"\"\n",
        "\n",
        "import os, json, datetime, unicodedata, re\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "# --- 1. SETTINGS & EXTERNAL CONFIG ---\n",
        "SETTINGS = {\n",
        "    \"INPUT_FILE\": \"data/input_data.xlsx\",\n",
        "    \"OUTPUT_BASENAME\": \"Cleaned_Dataset\",\n",
        "    \"FUZZY_THRESHOLD\": 87,\n",
        "    \"DIACRITIC_MODE\": \"broad\",\n",
        "}\n",
        "\n",
        "def load_config(file_path: str, default_val):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f) if file_path.endswith('.json') else pd.read_csv(file_path)\n",
        "    return default_val\n",
        "\n",
        "# Load Externalized data files\n",
        "HARMONIZATION_RULES = load_config(\"data/harmonization_map.csv\", pd.DataFrame())\n",
        "PARENT_PATTERNS = load_config(\"data/parent_patterns.json\", {})\n",
        "COMPANY_CUES = load_config(\"data/company_cues.json\", [\"oy\", \"ab\", \"inc\", \"ltd\"])\n",
        "\n",
        "# --- 2. TEXT NORMALIZATION ---\n",
        "def normalize_unicode(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = s.replace(\"–\", \"-\").replace(\"’\", \"'\").replace(\"”\", '\"')\n",
        "    return unicodedata.normalize(\"NFKC\", s)\n",
        "\n",
        "def restore_diacritics(token: str) -> str:\n",
        "    if SETTINGS[\"DIACRITIC_MODE\"] == \"broad\":\n",
        "        token = re.sub(r\"oe\", \"ö\", token)\n",
        "        token = re.sub(r\"ae\", \"ä\", token)\n",
        "    return token\n",
        "\n",
        "# --- 3. CORE CLEANING ENGINE ---\n",
        "def clean_token(token: str) -> Optional[str]:\n",
        "    if not token: return None\n",
        "    x = normalize_unicode(token).lower()\n",
        "    x = re.sub(r\"\\([^)]*\\)\", \" \", x)\n",
        "    suffix_pattern = r\"\\b(oy|oyj|ab|abp|gmbh|ltd|inc|plc|corp|sa|spa|as|aps)\\b\"\n",
        "    x = re.sub(suffix_pattern, \" \", x, flags=re.IGNORECASE)\n",
        "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
        "    return restore_diacritics(x) or None\n",
        "\n",
        "# --- 4. CLASSIFICATION & HIERARCHY ---\n",
        "def classify_entity(name: str) -> str:\n",
        "    n = name.lower()\n",
        "    if any(cue in n for cue in [\"university\", \"college\", \"institute\"]): return \"academia\"\n",
        "    if any(cue in n for cue in [\"foundation\", \"stiftung\", \"säätiö\"]): return \"nonprofit\"\n",
        "    if any(cue in n for cue in COMPANY_CUES): return \"company\"\n",
        "    return \"individual\"\n",
        "\n",
        "def detect_parent(name: str) -> str:\n",
        "    for parent, patterns in PARENT_PATTERNS.items():\n",
        "        if any(re.search(p, name, re.I) for p in patterns):\n",
        "            return parent\n",
        "    return name\n",
        "\n",
        "def run_pipeline():\n",
        "    try:\n",
        "        df = pd.read_excel(SETTINGS[\"INPUT_FILE\"])\n",
        "        for col in [\"Owners\", \"Applicants\"]:\n",
        "            if col in df.columns:\n",
        "                df[f\"{col.lower()}_clean\"] = df[col].apply(\n",
        "                    lambda x: [clean_token(p) for p in str(x).split(\";\")]\n",
        "                )\n",
        "\n",
        "        df[\"category\"] = df[\"owners_clean\"].apply(lambda lst: [classify_entity(x) for x in lst if x])\n",
        "        df[\"parent_entity\"] = df[\"owners_clean\"].apply(lambda lst: [detect_parent(x) for x in lst if x])\n",
        "\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "        df.to_excel(f\"{SETTINGS['OUTPUT_BASENAME']}_{timestamp}.xlsx\", index=False)\n",
        "        print(f\"Success: Processed {len(df)} rows.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Could not find {SETTINGS['INPUT_FILE']}. Please ensure data is in the /data folder.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "id": "aTErKNp9v9qZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}