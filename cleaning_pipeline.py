# -*- coding: utf-8 -*-
"""owner_cleaning_main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1McBPSkG-iko3X6ZlenvLOqUzRtYGDNVo

# Patent Owner Cleaning & Harmonisation Pipeline

**Purpose**: Clean and harmonise patent assignee/owner names exported from Lens.org; classify owner types; detect parent companies; apply fuzzy consolidation; export audit artefacts; and version the dataset with a reproducible ID & changelog.

**Design**: Hybrid multi‑cell structure for readability and reproducibility.

**Sections**
1. Settings & Imports  
2. Versioning Utilities (Dataset ID, Change Log)  
3. Helper Functions – Text Normalisation & Diacritics  
4. Helper Functions – Legal Suffix Removal & Owner Splitting  
5. Harmonisation Map (manual rules + optional external CSV)  
6. Parent Detection (patterns + optional external JSON)  
7. Fuzzy Matching & Near‑Duplicate Tools  
8. Main Pipeline (load → transform → export)

## 1) Settings & Imports
"""

import sys
!{sys.executable} -m pip install rapidfuzz

# Core imports
import os, json, csv, hashlib, datetime, unicodedata, re
import pandas as pd
from typing import List, Dict, Optional

# Fuzzy matching imports
from difflib import SequenceMatcher
try:
    import rapidfuzz.fuzz as fuzz
    HAVE_RAPIDFUZZ = True
except ImportError:
    HAVE_RAPIDFUZZ = False
print("HAVE_RAPIDFUZZ:", HAVE_RAPIDFUZZ)

# I/O defaults (edit these for your run)
INPUT_FILE = "groupA_incl_applicants.xlsx"            # Lens export (xlsx)
OWNERS_COL = "Owners"                 # Column with assignee/owners
APPLICANTS_COL = "Applicants"         # New column for applicants
OUTPUT_BASENAME = "GroupA_incl_applicants_cleaned"    # Base name for outputs

# Optional specs to lock search architecture for reproducibility
SEARCH_SPEC_FILE = "search_spec.json"  # store search terms, CPC lists, date filters

# Cleaning & matching parameters
DIACRITIC_MODE = "broad"    # "safe" or "broad"
FUZZY_THRESHOLD = 87        # 85–92 typical; higher = stricter
APPLY_FUZZY_TO_MAIN = False # if True, replaces owners_clean with fuzzy canonical
LIST_JOINER = "; "         # string joiner when exporting list columns

"""## 2) Versioning Utilities (Dataset ID & Change Log)"""

# Deterministic dataset versioning & changelog

CHANGELOG_CSV = "changelog.csv"

def file_md5(path: str, block_size: int = 1<<20) -> str:
    h = hashlib.md5()
    with open(path, 'rb') as f:
        for chunk in iter(lambda: f.read(block_size), b''):
            h.update(chunk)
    return h.hexdigest()

# Load optional search spec (to hash search architecture)
search_spec = {}
if os.path.exists(SEARCH_SPEC_FILE):
    with open(SEARCH_SPEC_FILE, "r", encoding="utf-8") as f:
        search_spec = json.load(f)

input_hash = file_md5(INPUT_FILE) if os.path.exists(INPUT_FILE) else "noinput"
spec_hash  = hashlib.md5(json.dumps(search_spec, sort_keys=True).encode("utf-8")).hexdigest() if search_spec else "nospec"
DATASET_ID = f"{input_hash[:8]}__{spec_hash[:8]}"

def hash_dataframe_content(df: pd.DataFrame) -> str:
    """
    Generates an MD5 hash of the DataFrame's content for content-based versioning.
    Converts DataFrame to a canonical CSV string to ensure consistent hashing.
    """
    # Create a copy to avoid modifying the original DataFrame
    df_copy = df.copy()

    # Convert list columns to sorted, joined strings for hashing consistency
    for col in df_copy.columns:
        # Check if the column contains lists and is not already a string column
        if df_copy[col].apply(lambda x: isinstance(x, list)).any():
            df_copy[col] = df_copy[col].apply(lambda x: LIST_JOINER.join(sorted(x)) if isinstance(x, list) else x)

    # Sort by all columns to ensure consistent order for hashing
    df_sorted = df_copy.sort_values(by=list(df_copy.columns)).reset_index(drop=True)

    # Convert to CSV string with consistent formatting
    # Using float_format to ensure consistent representation of floats
    # Using date_format for consistent date representation
    # Replacing NaN with a consistent string (e.g., empty string)
    csv_string = df_sorted.to_csv(index=False,
                                  float_format='%.8f', # Standardize float precision
                                  date_format='%Y-%m-%dT%H:%M:%S', # Standardize date format
                                  na_rep='').encode('utf-8')
    return hashlib.md5(csv_string).hexdigest()

print("DATASET_ID:", DATASET_ID)

# helper to append a row to changelog
_def_fields = [
    "dataset_id","input_file","owners_col","rows","unique_raw_owners","unique_clean_owners",
    "fuzzy_threshold","apply_fuzzy_to_main","diacritic_mode","timestamp"
]

def write_changelog_row(summary: dict):
    file_exists = os.path.exists(CHANGELOG_CSV)
    with open(CHANGELOG_CSV, "a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=_def_fields)
        if not file_exists:
            w.writeheader()
        # ensure all fields exist
        row = {k: summary.get(k, "") for k in _def_fields}
        w.writerow(row)

"""## 3) Helper Functions – Text Normalisation & Diacritics"""

# --- Unicode normalisation and diacritic restoration ---

def normalize_unicode(s: str) -> str:
    if s is None or not isinstance(s, str):
        return ""
    # normalise punctuation but keep Nordic letters
    s = (s.replace("–", "-")
           .replace("—", "-")
           .replace("‚", "'")
           .replace("’", "'")
           .replace("”", '"')
           .replace("“", '"'))
    return unicodedata.normalize("NFKC", s)

# Known safe stems for exact replacements
NORDIC_DIACRITIC_STEMS = {
    "ahlström": {"ahlstrom", "ahlstroem", "ahlstrm", "ahlström"},
    "munksjö": {"munksjo", "munksjoe", "munksjö"},
    "göran": {"goran", "goeran", "göran"},
    "jörgensen": {"jorgensen", "joergensen", "jörgensen"},
    "säätiö": {"saatio", "saeaetio", "säätiö"},
}
SAFE_VARIANT_TO_CANON = {v: k for k, vs in NORDIC_DIACRITIC_STEMS.items() for v in vs}

def restore_diacritics_safe(token: str) -> str:
    if not token:
        return token
    if token in SAFE_VARIANT_TO_CANON:
        return SAFE_VARIANT_TO_CANON[token]
    if "-" in token:
        parts = token.split("-")
        fixed = [SAFE_VARIANT_TO_CANON.get(p, p) for p in parts]
        return "-".join(fixed)
    return token

_DIA_BROAD_PATTERNS = [
    (re.compile(r"oe"), "ö"),
    (re.compile(r"ae"), "ä"),
    (re.compile(r"sjo"), "sjö"),
]

def restore_diacritics_broad(token: str) -> str:
    if not token:
        return token
    t = restore_diacritics_safe(token)
    for pat, rep in _DIA_BROAD_PATTERNS:
        t = pat.sub(rep, t)
    return t

def restore_diacritics(token: str) -> str:
    return restore_diacritics_safe(token) if DIACRITIC_MODE == "safe" else restore_diacritics_broad(token)

"""## 4) Helper Functions – Legal Suffix Removal & Owner Splitting"""

import os
import json
import re
from typing import List, Optional

# 1. Load configuration
CUES_FILE = "classification_cues.json"
DEFAULT_SUFFIXES = ["inc", "ltd", "oy", "ab", "gmbh", "l.p", "n.v"]

if os.path.exists(CUES_FILE):
    try:
        with open(CUES_FILE, "r", encoding="utf-8") as f:
            config = json.load(f)
            # Use raw suffixes from JSON (allowing your regex like [oö] to work)
            raw_suffixes = config.get("legal_suffixes", DEFAULT_SUFFIXES)
    except Exception as e:
        print(f"Error loading {CUES_FILE}: {e}")
        raw_suffixes = DEFAULT_SUFFIXES
else:
    raw_suffixes = DEFAULT_SUFFIXES

# 2. Build a robust Regex for suffixes
# We do NOT use re.escape here because your JSON already contains regex escapes like \\.
# We wrap them in \b for word boundaries.
processed_suffixes = [rf"\b{s}(?:\W|$)" for s in raw_suffixes]
LEGAL_SUFFIXES_RE = re.compile(r"(?i)" + "|".join(processed_suffixes))

# 3. Clean-up Patterns
PRIMARY_SEP_RE = re.compile(r"[;|/•]|(?:\s+&\s+)|\s+\band\b\s+", flags=re.IGNORECASE)
PARENS_RE = re.compile(r"\([^)]*\)")
# We allow dots temporarily so suffixes like l.p can be matched
ALLOWED_CHARS_WITH_DOTS = re.compile(r"[^a-z0-9\s.&\-åäöæø]")
# Final pass to remove dots and other junk
FINAL_CLEANUP_RE = re.compile(r"[^a-z0-9\s&\-åäöæø]")

def strip_parentheses(s: str) -> str:
    return PARENS_RE.sub(" ", s)

def normalize_owner_token(token: str) -> Optional[str]:
    if token is None or not isinstance(token, str):
        return None

    # --- PHASE 1: PRE-CLEANING ---
    x = normalize_unicode(token)
    x = strip_parentheses(x)
    x = x.lower()

    # Remove "the" as a whole word
    x = re.sub(r"\bthe\b", "", x)

    # Remove non-essential symbols but KEEP DOTS for suffix matching
    x = ALLOWED_CHARS_WITH_DOTS.sub(" ", x)

    # --- PHASE 2: SUFFIX REMOVAL ---
    # Matches "l.p", "inc.", etc. while the dots still exist
    x = LEGAL_SUFFIXES_RE.sub(" ", x)

    # --- PHASE 3: FINAL MOP-UP ---
    # Now remove all remaining dots and special chars
    x = FINAL_CLEANUP_RE.sub(" ", x)

    # Standardize conjunctions
    x = x.replace("&", " and ")
    x = re.sub(r"\s*-\s*", "-", x)

    # Remove trailing/leading punctuation and multiple spaces
    x = re.sub(r"[-.,;\s]+$", "", x)
    x = re.sub(r"^\s+", "", x)
    x = re.sub(r"\s+", " ", x).strip()

    # Discard single-character noise
    if len(x) < 2 or not any(c.isalnum() for c in x):
        return None

    return restore_diacritics(x)

def split_owners_cell(cell: str) -> List[str]:
    """
    Splits a raw string of owners/applicants into a list based on
    common delimiters like semicolons, slashes, or 'and'.
    """
    if not cell or not isinstance(cell, str):
        return []

    # Use the regex defined in your notebook or a standard multi-delimiter split
    # PRIMARY_SEP_RE was defined in your Section 4 block
    parts = PRIMARY_SEP_RE.split(cell)

    # Clean up whitespace for each split part
    return [p.strip() for p in parts if p.strip()]

"""## 5) Harmonisation Map (manual rules + optional external CSV)"""

# Manual harmonisation rules (you can extend this in 'owners_harmonization.csv')
HARMONIZATION_MAP = {
    # Core examples (add more in CSV without touching code)
}

# Optional: owners_harmonization.csv with columns: variant, canonical
CSV_RULES_FILE = "owners_harmonization.csv"
if os.path.exists(CSV_RULES_FILE):
    try:
        _df_rules = pd.read_csv(CSV_RULES_FILE)
        for _, r in _df_rules.iterrows():
            v = str(r.get("variant", "")).strip().lower()
            c = str(r.get("canonical", "")).strip().lower()
            if v and c:
                HARMONIZATION_MAP[v] = c
        print(f"Loaded extra harmonization rules: {len(_df_rules)}")
    except Exception as e:
        print("Could not read owners_harmonization.csv:", e)


def harmonize_owner(clean_name: str) -> Optional[str]:
    if not clean_name:
        return None
    return HARMONIZATION_MAP.get(clean_name, clean_name)


def process_owners_cell(cell) -> List[str]:
    if pd.isna(cell):
        cell = ""
    elif not isinstance(cell, str):
        cell = str(cell)
    parts = split_owners_cell(cell)
    cleaned = []
    for p in parts:
        c = normalize_owner_token(p)
        if c:
            c = harmonize_owner(c)
            cleaned.append(c)
    # de-duplicate, preserve order
    deduped, seen = [], set()
    for c in cleaned:
        if c not in seen:
            deduped.append(c)
            seen.add(c)
    return deduped

"""## 6) Parent Detection (patterns + optional external JSON)"""

# Base parent patterns (extendable via parent_patterns.json)
PARENT_PATTERNS = {
    "stora enso": [r"\bstora\s*enso\b", r"\bstora[-\s]?enso\b", r"\bstora\b.*\benso\b"],
    "upm": [r"\bupm\b", r"\bupm[-\s]?kymmene\b"],
    "metsä group": [r"\bmets[aä]\s*group\b", r"\bmets[aä]\s*(board|tissue|fibre|fiber)\b"],
    "tetra laval": [r"\btetra\s*laval\b"],
    "stora enso oyj": [r"\bstora\s*enso\s*oyj\b"],
}

# Optional external JSON to override/extend patterns
PARENT_PATTERNS_FILE = "parent_patterns.json"
if os.path.exists(PARENT_PATTERNS_FILE):
    try:
        with open(PARENT_PATTERNS_FILE, "r", encoding="utf-8") as f:
            extra = json.load(f)
            for k, lst in extra.items():
                PARENT_PATTERNS.setdefault(k, [])
                PARENT_PATTERNS[k].extend(lst)
        print("Loaded parent_patterns.json; parents:", len(PARENT_PATTERNS))
    except Exception as e:
        print("Could not read parent_patterns.json:", e)

GENERIC_ORG_WORDS = [
    "company", "co", "corporation", "corp", "limited", "ltd", "llc", "plc",
    "group", "holding", "holdings", "konserni", "konsern", "konzern",
    "osakeyhtiö", "oy", "oyj", "ab", "abp", "aktiebolag", "ag", "gmbh", "kg",
    "se", "kgaa", "as", "asa", "aps", "a/s", "sa", "sarl", "spa", "sas"
]
GENERIC_ORG_RE = re.compile(r"(?i)" + "\b(" + "|".join(map(re.escape, GENERIC_ORG_WORDS)) + r")\b")


def collapse_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()


def token_root(name: str) -> str:
    n = name.lower()
    n = GENERIC_ORG_RE.sub(" ", n)
    n = collapse_spaces(n)
    tokens = n.split()
    if not tokens:
        return n
    if "stora" in tokens and "enso" in tokens:
        return "stora enso"
    return " ".join(tokens[:2])


def detect_parent_for_owner(owner: str) -> Optional[str]:
    n = owner.lower()
    # explicit patterns
    for parent, patterns in PARENT_PATTERNS.items():
        for pat in patterns:
            if re.search(pat, n, flags=re.IGNORECASE):
                return parent
    # heuristics
    root = token_root(owner)
    # Replaced hardcoded set with dynamic check against keys in PARENT_PATTERNS
    if root in PARENT_PATTERNS:
        return root
    if re.search(r"\bmets[aä]\b", n):
        return "metsä group"
    return None

"""## 7) Fuzzy Matching & Near‑Duplicate Tools"""

# Fuzzy helpers

def _token_sort(s: str) -> str:
    toks = re.findall(r"[a-z0-9]+", s.lower())
    return " ".join(sorted(toks))

BAD_PIVOT_TOKENS = {"company","group","holding","holdings","inc","co","corp","ltd","oy","ab","ag","gmbh"}

def _similarity(a: str, b: str) -> float:
    if HAVE_RAPIDFUZZ:
        return fuzz.token_sort_ratio(a, b)  # 0..100
    return SequenceMatcher(None, _token_sort(a), _token_sort(b)).ratio() * 100.0

def is_bad_pivot(name: str) -> bool:
    toks = set(re.findall(r"[a-z0-9]+", name))
    return len(toks - BAD_PIVOT_TOKENS) <= 1


def build_fuzzy_map(canonical_list: List[str], threshold: int = 87) -> Dict[str, str]:
    mapping = {}
    uniques = [n for n in canonical_list if n and not is_bad_pivot(n)]
    for name in uniques:
        for other in uniques:
            if name == other:
                continue
            score = _similarity(name, other)
            if score >= threshold:
                mapping[other] = name
    return mapping


def apply_fuzzy_to_list(names: List[str], fmap: Dict[str, str]) -> List[str]:
    out = [fmap.get(n, n) for n in names]
    seen, dedup = set(), []
    for x in out:
        if x not in seen:
            dedup.append(x)
            seen.add(x)
    return dedup

# Near‑duplicate review table

def near_dupe_table(uniques: List[str], fmap: Dict[str, str]) -> pd.DataFrame:
    rows = []
    by_canon = {}
    for variant, canon in fmap.items():
        by_canon.setdefault(canon, set()).add(variant)
    for canon, variants in by_canon.items():
        for v in sorted(list(variants)):
            if v == canon:
                continue
            score = _similarity(canon, v)
            rows.append({"variant": v, "canonical": canon, "score": round(score, 1)})
    if not rows:
        return pd.DataFrame(columns=["variant", "canonical", "score"])
    return pd.DataFrame(rows).sort_values(["canonical", "score"], ascending=[True, False]).drop_duplicates()

"""## 8) Main Pipeline (load → transform → export)"""

# Load data
assert os.path.exists(INPUT_FILE), f"Input file not found: {INPUT_FILE}"
df = pd.read_excel(INPUT_FILE, engine="openpyxl")
if OWNERS_COL not in df.columns:
    raise KeyError(f"Column '{OWNERS_COL}' not found. Available columns: {list(df.columns)}")

HAS_APPLICANTS_COL = APPLICANTS_COL in df.columns
if not HAS_APPLICANTS_COL:
    print(f"Warning: Column '{APPLICANTS_COL}' not found. Applicant-related processing will be skipped.")

print(f"Loaded rows: {len(df):,}")

# 1) Clean owners and applicants into lists
df["owners_clean"] = df[OWNERS_COL].apply(process_owners_cell)
if HAS_APPLICANTS_COL:
    df["applicants_clean"] = df[APPLICANTS_COL].apply(process_owners_cell)
else:
    df["applicants_clean"] = [[] for _ in range(len(df))]

# Removed: 2) Owner/Applicant type per cleaned entry
# df["owner_category"] = df["owners_clean"].apply(lambda lst: [classify_single_owner(x) for x in lst])
# if HAS_APPLICANTS_COL:
#     df["applicant_category"] = df["applicants_clean"].apply(lambda lst: [classify_single_owner(x) for x in lst])
# else:
#     df["applicant_category"] = [[] for _ in range(len(df))]

# 3) Parent detection per cleaned entry (fallback to self to enable grouping later)
df["owners_parent"] = df["owners_clean"].apply(lambda lst: [detect_parent_for_owner(x) or x for x in lst])
if HAS_APPLICANTS_COL:
    df["applicants_parent"] = df["applicants_clean"].apply(lambda lst: [detect_parent_for_owner(x) or x for x in lst])
else:
    df["applicants_parent"] = [[] for _ in range(len(df))]

# 4) Fuzzy map built on unique cleaned owners AND applicants (after manual harmonization)
# Combine unique cleaned owners and applicants for fuzzy matching
all_clean_names = pd.concat([df["owners_clean"].explode(), df["applicants_clean"].explode()]).dropna().unique()
unique_clean = list(all_clean_names)
print("Unique cleaned owners and applicants:", len(unique_clean))

fuzzy_map = build_fuzzy_map(unique_clean, threshold=FUZZY_THRESHOLD)

# 5) Apply fuzzy canonical to each row (non-destructive; separate column)
df["owners_clean_fuzzy"] = df["owners_clean"].apply(lambda lst: apply_fuzzy_to_list(lst, fuzzy_map))
if HAS_APPLICANTS_COL:
    df["applicants_clean_fuzzy"] = df["applicants_clean"].apply(lambda lst: apply_fuzzy_to_list(lst, fuzzy_map))
else:
    df["applicants_clean_fuzzy"] = [[] for _ in range(len(df))]

# 6) Optionally replace owners_clean with fuzzy-canonical for owners_final and applicants_final
if APPLY_FUZZY_TO_MAIN:
    df["owners_final"] = df["owners_clean_fuzzy"]
    df["applicants_final"] = df["applicants_clean_fuzzy"]
else:
    df["owners_final"] = df["owners_clean"]
    df["applicants_final"] = df["applicants_clean"]

# Create a combined_final column prioritizing owners_final, then applicants_final
def combine_owners_applicants(row):
    if row["owners_final"] and any(row["owners_final"]):
        return row["owners_final"]
    elif row["applicants_final"] and any(row["applicants_final"]):
        return row["applicants_final"]
    return []

df['combined_final'] = df.apply(combine_owners_applicants, axis=1)

# 7) Human-friendly string columns for Excel
for col_in, col_out in [
    ("owners_clean", "owners_clean_str"),
    ("owners_clean_fuzzy", "owners_clean_fuzzy_str"),
    ("owners_parent", "owners_parent_str"),
    ("owners_final", "owners_final_str"),
    ("applicants_clean", "applicants_clean_str"),
    ("applicants_clean_fuzzy", "applicants_clean_fuzzy_str"),
    ("applicants_parent", "applicants_parent_str"),
    ("applicants_final", "applicants_final_str"),
    ("combined_final", "combined_final_str"), # New combined column
]:
    df[col_out] = df[col_in].apply(lambda lst: LIST_JOINER.join(lst) if isinstance(lst, list) else "")

# --- REVIEW / AUDIT TABLES ---

def build_mapping(raw_series, clean_series):
    rows = []
    for raw, clist in zip(raw_series, clean_series):
        raw_norm = normalize_unicode(raw) if isinstance(raw, str) else raw
        if isinstance(clist, list):
            for c in clist:
                rows.append({"raw_cell": raw_norm, "clean_owner": c})
    return pd.DataFrame(rows).drop_duplicates().sort_values(["clean_owner", "raw_cell"])

mapping_df = build_mapping(df[OWNERS_COL], df["owners_clean"])

applicants_mapping_df = pd.DataFrame(columns=["raw_cell", "clean_owner"]) # Initialize empty if no applicants column
if HAS_APPLICANTS_COL:
    applicants_mapping_df = build_mapping(df[APPLICANTS_COL], df["applicants_clean"])

# Owner -> Parent (unique)
owner_parent_rows = []
for owners, parents in zip(df["owners_clean"], df["owners_parent"]):
    for o, p in zip(owners, parents):
        owner_parent_rows.append({"clean_owner": o, "parent_detected": p})
owner_parent_df = pd.DataFrame(owner_parent_rows).drop_duplicates().sort_values(["parent_detected", "clean_owner"])

# Applicant -> Parent (unique)
applicant_parent_rows = []
if HAS_APPLICANTS_COL:
    for applicants, parents in zip(df["applicants_clean"], df["applicants_parent"]):
        for a, p in zip(applicants, parents):
            applicant_parent_rows.append({"clean_applicant": a, "parent_detected": p})
    # Corrected indentation here
    applicant_parent_df = pd.DataFrame(applicant_parent_rows).drop_duplicates().sort_values(["parent_detected", "clean_applicant"])
else:
    applicant_parent_df = pd.DataFrame(columns=["clean_applicant", "parent_detected"])

# Owners that didn't map to a different parent (parent==self)
unresolved_df = owner_parent_df[owner_parent_df["clean_owner"] == owner_parent_df["parent_detected"]].copy()
unresolved_df = unresolved_df.sort_values(["clean_owner"])

# Applicants that didn't map to a different parent (parent==self)
unresolved_applicants_df = pd.DataFrame(columns=["clean_applicant", "parent_detected"])
if HAS_APPLICANTS_COL:
    unresolved_applicants_df = applicant_parent_df[applicant_parent_df["clean_applicant"] == applicant_parent_df["parent_detected"]].copy()
    unresolved_applicants_df = unresolved_applicants_df.sort_values(["clean_applicant"])

# Near-duplicate review with similarity score
near_dupes_df = near_dupe_table(list(unique_clean), fuzzy_map)

# Rule suggestions for HARMONIZATION_MAP additions
# Filter out suggestions where the variant is already a canonical in HARMONIZATION_MAP
rules_df = near_dupes_df.loc[
    (near_dupes_df["variant"] != near_dupes_df["canonical"]) &
    (~near_dupes_df["variant"].isin(HARMONIZATION_MAP.keys())),
    ["variant", "canonical"]
].rename(columns={"canonical": "suggested_canonical"}).drop_duplicates()

# --- EXPORTS ---

# List of DataFrames to export and their base filenames
export_dfs = [
    (df, OUTPUT_BASENAME),
    (mapping_df, "owners_mapping_review"),
    (applicants_mapping_df, "applicants_mapping_review"),
    (owner_parent_df, "owners_parent_groups"),
    (applicant_parent_df, "applicants_parent_groups"),
    (unresolved_df, "owners_without_parent"),
    (unresolved_applicants_df, "applicants_without_parent"),
    (near_dupes_df, "owners_applicants_near_duplicates"),
    (rules_df, "owners_applicants_rule_suggestions"),
]

output_messages = []
for current_df, base_filename in export_dfs:
    if current_df.empty and ("applicants" in base_filename or "applicant" in base_filename):
        # Skip empty applicant-related dataframes if there were no applicants
        if not HAS_APPLICANTS_COL:
            output_messages.append(f"  - Skipped empty file: {base_filename}__{DATASET_ID}__<content_hash>.xlsx (no applicants)")
            continue

    if current_df.empty:
        # Skip other empty dataframes, might not be an error
        output_messages.append(f"  - Skipped empty file: {base_filename}__{DATASET_ID}__<content_hash>.xlsx")
        continue

    # Calculate content hash
    content_hash = hash_dataframe_content(current_df)

    # Construct the final output filename with content hash
    final_output_filename = f"{base_filename}__{DATASET_ID}__{content_hash[:8]}.xlsx"

    # Check if file already exists with this exact content hash
    if os.path.exists(final_output_filename):
        output_messages.append(f"  - Skipped (content identical): {final_output_filename}")
    else:
        current_df.to_excel(final_output_filename, index=False)
        output_messages.append(f"  - Created: {final_output_filename}")

print("\u2713 Owners and Applicants cleaned and enriched.")
for msg in output_messages:
    print(msg)
print(f"Fuzzy matching via {'RapidFuzz' if HAVE_RAPIDFUZZ else 'difflib fallback'}. Threshold = {FUZZY_THRESHOLD}")
print(f"APPLY_FUZZY_TO_MAIN = {APPLY_FUZZY_TO_MAIN}")

# --- CHANGELOG ENTRY ---
summary = {
    "dataset_id": DATASET_ID,
    "input_file": INPUT_FILE,
    "owners_col": OWNERS_COL,
    "applicants_col": APPLICANTS_COL if HAS_APPLICANTS_COL else "Not Present",
    "rows": int(df.shape[0]),
    "unique_raw_owners": int(df[OWNERS_COL].nunique()),
    "unique_raw_applicants": int(df[APPLICANTS_COL].nunique()) if HAS_APPLICANTS_COL else 0,
    "unique_clean_owners_and_applicants": int(len(unique_clean)),
    "fuzzy_threshold": FUZZY_THRESHOLD,
    "apply_fuzzy_to_main": APPLY_FUZZY_TO_MAIN,
    "diacritic_mode": DIACRITIC_MODE,
    "timestamp": datetime.datetime.now().isoformat(timespec='seconds')
}
write_changelog_row(summary)
print("Changelog updated:", CHANGELOG_CSV)